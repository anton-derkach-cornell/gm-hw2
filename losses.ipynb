{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5373bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e615b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def gaussian_elbo(x1,x2,z,sigma,mu,logvar):\n",
    "    \n",
    "    #\n",
    "    # Problem 5b: Compute the evidence lower bound for the Gaussian VAE.\n",
    "    #             Use the closed-form expression for the KL divergence from Problem 1.\n",
    "    #\n",
    "\n",
    "    # reconstruction = (1 / (2 * sigma**2)) * torch.sum((x2 - x1)**2)\n",
    "    # divergence = 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar)\n",
    "\n",
    "    # return reconstruction, divergence\n",
    "    \n",
    "    reconstruction = (1./(2*sigma**2))(x1 - x2).pow(2).view(x1.shape[0],-1).sum(1).mean()\n",
    "    divergence = .5*(logvar.exp() + mu.pow(2) - 1 - logvar).sum(1).mean()\n",
    "    return reconstruction, divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1abdc2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mc_gaussian_elbo(x1,x2,z,sigma,mu,logvar):\n",
    "\n",
    "    #\n",
    "    # Problem 5c: Compute the evidence lower bound for the Gaussian VAE.\n",
    "    #             Use a (1-point) monte-carlo estimate of the KL divergence.\n",
    "    #\n",
    "\n",
    "    # reconstruction = (1 / (2 * sigma**2)) * torch.sum((x2 - x1)**2)\n",
    "    \n",
    "    # log_q = -0.5 * (torch.sum(logvar, dim=1) + torch.sum((z - mu)**2 / torch.exp(logvar), dim=1))\n",
    "    # log_r = -0.5 * torch.sum(z**2, dim=1)\n",
    "    \n",
    "    # divergence = torch.sum(log_q - log_r)\n",
    "\n",
    "    # return reconstruction, divergence\n",
    "\n",
    "    reconstruction = (1./sigma*2)*(x1 - x2).pow(2).view(x1.shape[0],-1).mean()\n",
    "    logpz = 0.5 * z.pow(2).sum(1)\n",
    "    logqzx = 0.5 * (logvar + torch.pow((z-mu),2)/logvar.exp()).sum(1)\n",
    "    divergence = (logpz - logqzx).mean()\n",
    "    return reconstruction, divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60406a80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cross_entropy(x1,x2):\n",
    "    return F.binary_cross_entropy_with_logits(x1, x2, reduction='sum')/x1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_output_elbo(x1,x2,z,logqzx):\n",
    "\n",
    "    #\n",
    "    # Problem 6b: Compute the evidence lower bound for a VAE with binary outputs.\n",
    "    #             Use a (1-point) monte carlo estimate of the KL divergence.\n",
    "    #\n",
    "\n",
    "\n",
    "    reconstruction = cross_entropy(x1, x2)\n",
    "\n",
    "    #    Here, logqzx = - log q(z|x) ignoring the constant,\n",
    "    #    so we also compute - log p(z) ignoring the same constant.\n",
    "    #       - log p(z) = 0.5 * ||z||^2\n",
    "    \n",
    "    logp_z = 0.5 * (z ** 2).sum(dim=1)  # shape (batch_size,)\n",
    "    divergence = (logp_z - logqzx).mean()\n",
    "    \n",
    "    return reconstruction, divergence"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
